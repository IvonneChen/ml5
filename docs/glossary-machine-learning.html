<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Machine Learning · ML5.js</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Machine Learning · ML5.js"/><meta property="og:type" content="website"/><meta property="og:url" content="https://itpnyu.github.io/ml5/index.html"/><meta property="og:description" content="## The data set"/><link rel="shortcut icon" href="/ml5/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css"/><script type="text/javascript" src="https://rawgit.com/ITPNYU/ml5/master/dist/ml5.min.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.5.16/p5.min.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.5.16/addons/p5.dom.min.js"></script><link rel="stylesheet" href="/ml5/css/main.css"/></head><body class="sideNavVisible"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/ml5/"><h2 class="headerTitle">ML5.js</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li><a href="/ml5/docs/getting-started.html" target="_self">API</a></li><li><a href="/ml5/docs/simple-image-classification-example.html" target="_self">Examples</a></li><li><a href="/ml5/en/experiments.html" target="_self">Experiments</a></li><li><a href="/ml5/docs/glossary-statistics.html" target="_self">Learn</a></li><li><a href="https://github.com/ITPNYU/ml5" target="_self">Code</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Glossary</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Glossary</h3><ul><li class="navListItem"><a class="navItem" href="/ml5/docs/glossary-statistics.html">Statistics</a></li><li class="navListItem"><a class="navItem" href="/ml5/docs/glossary-mathematics.html">Mathematics</a></li><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/ml5/docs/glossary-machine-learning.html">Machine Learning</a></li></ul></div><div class="navGroup navGroupActive"><h3>Tutorials</h3><ul><li class="navListItem"><a class="navItem" href="/ml5/docs/tutorials.html">Tutorials</a></li></ul></div><div class="navGroup navGroupActive"><h3>Resources</h3><ul><li class="navListItem"><a class="navItem" href="/ml5/docs/resources.html">Resources</a></li></ul></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1>Machine Learning</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" name="the-data-set"></a><a href="#the-data-set" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The data set</h2>
<p>The accessible sample is our data set, which consists of a number of <em>data points</em>. Each data point will contain information expressed through a number of attributes. Each attribute has a <em>data type</em>. The most common data types are numbers (<em>numerical data</em>) and text strings (<em>nominal data</em>). A subset of nominal data is <em>categorical</em>, i.e. from a finite set of options, such as &quot;yes/no&quot;, month, etc. Categorical data are sometimes known as <em>factors</em>.</p>
<h2><a class="anchor" aria-hidden="true" name="features"></a><a href="#features" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Features</h2>
<p>A feature is an attribute that is used in the machine learning analysis of the problem. This could be the raw features, but may also be quantities extracted by some kind of pre-processing of the data. Extracting appropriate features is known as <em>feature engineering</em>. Feature engineering traditionally requires some kind of <em>domain knowledge</em>. One of the strengths of deep learning is, that it essentially does the feature engineering for you: The model itself learns appropriate features from the raw data.</p>
<h2><a class="anchor" aria-hidden="true" name="models"></a><a href="#models" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models</h2>
<p>A <em>mathematical model</em> is an abstraction trying to describe some part of reality. Reality is complex, so a model will never be able to completely mimic real phenomena. But we should pick a model that catches the broad strokes of behaviour. Keep in mind the famous words of the great statistician George Box: &quot;All models are wrong, but some are useful&quot;.</p>
<h2><a class="anchor" aria-hidden="true" name="algorithms"></a><a href="#algorithms" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Algorithms</h2>
<p>A <em>machine learning algorithm</em> is a &quot;recipe&quot; that uses the data to make a model of a given type. A model is usually described by a number of <em>parameters</em>. For instance, a linear model y=ax+b has the slope a and the intercept b as parameters. The algorithm seeks out values of the parameters that are the best in some sense.</p>
<h2><a class="anchor" aria-hidden="true" name="supervised-learning"></a><a href="#supervised-learning" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Supervised learning</h2>
<p>Supervised learning is a class of algorithms, where we already know the attribute we wish to predict for the sample data set. For instance, we may have a large number of pictures of cats and dogs, where the animal in each photo as already been identified. Such an attribute is known as a <em>label</em>.</p>
<p>The two most important subcategories of supervised learning is:</p>
<h3><a class="anchor" aria-hidden="true" name="classification"></a><a href="#classification" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Classification</h3>
<p>Supervised learning, where the label is categorical. Like in the cat/dog example above. When, like here, there's only two possible outcomes, we speak of <em>binary classification</em>. An example is <em>k-nearest neighbors</em> (kNN) which looks at the k nearest data points and predicts based on the majority of these.</p>
<h3><a class="anchor" aria-hidden="true" name="regression"></a><a href="#regression" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Regression</h3>
<p>Supervised learning, where the label is numerical. <em>Simple linear regression</em> is an example.</p>
<h2><a class="anchor" aria-hidden="true" name="unsupervised-learning"></a><a href="#unsupervised-learning" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Unsupervised learning</h2>
<p>Unsupervised learning is the process of searching for <em>structure</em> in a data set. This often involves <em>clustering</em>, i.e. dividing the data set into groups that are similar in some respect. k-means clustering is the most famous example.</p>
<h2><a class="anchor" aria-hidden="true" name="subdividing-the-data-set"></a><a href="#subdividing-the-data-set" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Subdividing the data set</h2>
<p>To be able to validate a model learned by a supervised training algorithm, the data set is traditionally divided into two sets (at random) before doing the training: The <em>training set</em> and the <em>test</em> set. The idea is, that we must not use the test set in our model building. That way, we can see how it performs on a &quot;new&quot; data set - the test set. A typical split is 80% for the training set and 20% for the test set, but there's no hard and fast rules.</p>
<h3><a class="anchor" aria-hidden="true" name="training-set"></a><a href="#training-set" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training set</h3>
<p>This is the set for which the actual training algorithm is performed. So the model is made using only the data in the training set.</p>
<h3><a class="anchor" aria-hidden="true" name="test-set"></a><a href="#test-set" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test set</h3>
<p>The model resulting from the test set is used to make <em>predictions</em> for the data in the test set. These predictions are then compared to the actual labels from the test set. For categorical data, the results are often summed up in a <em>confusion matrix</em>.</p>
<h3><a class="anchor" aria-hidden="true" name="validation-set"></a><a href="#validation-set" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Validation set</h3>
<p>Some models have additional <em>hyperparameters</em> that needs to be tuned to find the best version of the model. A hyperparameter is typically a quantity which tweaks the way the &quot;ordinary&quot; model parameters are found. Examples are the k in k-nearest neighbors or the learning rate described below. It can be tempting to try out different values of these hyperparameters when using the test set to make predictions. However, if we do this, we violate the principle that only the training set should be used to build the model. Therefore, often the training set is further subdivided into (proper) training set and <em>validation set</em>. The model is then build using the training set, and the hyperparameter(s) are fitted by prediction on the validation set. Some methods randomly pick several validation sets as part of  learning. Such sets are known as <em>cross-validation</em> sets (but is often used synonymously with validation set). A common distribution between training and validation sets is 70% and 30%, but again these are guidelines.</p>
<h2><a class="anchor" aria-hidden="true" name="loss-function"></a><a href="#loss-function" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Loss function</h2>
<p>Many machine learning algorithms work through a <em>loss function</em>. A loss function (also known as a <em>cost function</em>) quantifies how badly a model performs on the training set. For instance, for numerical labels, it might sum over the squared differences between predicted and actual values. Hence, the smaller the loss function, the better the model.</p>
<h2><a class="anchor" aria-hidden="true" name="gradient-descent"></a><a href="#gradient-descent" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gradient descent</h2>
<p>So the objective is to minimize the loss function. If the gradients of the function can be calculated (or estimated numerically), a minimum can be searched for by starting with a random set of model parameters and the gradually moving in the direction where the loss functions decreases the most. This is known as <em>gradient descent</em>, and is similar to a ball rolling downhill in the parameter landscape. The size of these steps is proportional to the <em>learning rate</em> hyperparameter (also known as the <em>step size</em>). A large learning rate will make convergence faster, but sometimes has trouble finding the minimum, while a smaller learning rate is slower but more reliable. If the loss function is not <em>convex</em> in the parameters, the algorithm may get stuck at a local minimum. A number of modifications to this algorithm exists.</p>
<h2><a class="anchor" aria-hidden="true" name="over-and-underfitting"></a><a href="#over-and-underfitting" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Over- and underfitting</h2>
<p>A model that follows the training data very closely by fitting a large number of parameters has the risk of building the model on the noise in the training as well as the overall trends. This means, that while the model will perform excellently on the training set, it will usually perform poorly on the test set. This is known as <em>overfitting</em>. On the other hand, if there's not a lot of parameters to tune - or if the model is simply inappropriate to describe the underlying phenomenon - the model will always perform badly. This is known as <em>underfitting</em>. <em>Regularization</em> is a technique commonly used to avoid overfitting. It uses a hyperparameter to control the magnitude of the model parameters.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="glossary-mathematics.html">← Mathematics</a><a class="docs-next button" href="tutorials.html">Tutorials →</a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><h5>Docs</h5><a href="/ml5/docs/getting-started.html">Getting Started</a><a href="/ml5/docs/imagenet.html">API Reference</a><a href="/ml5/docs/training-models.html">Training Models</a></div><div><h5>Learning</h5><a href="/ml5/docs/tutorials.html">Tutorials</a><a href="/ml5/docs/glossary-statistics.html">Glossary</a><a href="/ml5/docs/resources.html">Resources</a></div><div><h5>Contribute</h5><a href="/ml5/experiments.html">Experiments</a><a href="https://github.com/ITPNYU/ml5">Contributing Guide</a><a class="github-button" href="https://github.com/ITPNYU/ml5" data-icon="octicon-star" data-count-href="https://github.com/ITPNYU/ml5" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://itp.nyu.edu" target="_blank" class="fbOpenSource"><img src="/ml5/img/itp_logo.png" alt="Facebook Open Source" width="60" height="45"/></a><section class="copyright">This project is currently being maintained at NYU ITP by a community of teachers, residents and students.</section></footer></div></body></html>